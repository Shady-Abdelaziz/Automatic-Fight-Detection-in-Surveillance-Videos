{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":397693,"sourceType":"datasetVersion","datasetId":176381},{"sourceId":4718786,"sourceType":"datasetVersion","datasetId":2730182}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.applications import MobileNetV3Small\nfrom tensorflow.keras import layers, models\nimport cv2\nimport os\nimport numpy as np\nfrom tensorflow.keras.callbacks import EarlyStopping , ReduceLROnPlateau\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\nfrom tensorflow.keras.applications import MobileNetV3Small\nfrom tensorflow.keras import Input\nprint('DONE')\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-30T00:08:09.557567Z","iopub.execute_input":"2024-11-30T00:08:09.557966Z","iopub.status.idle":"2024-11-30T00:08:21.055341Z","shell.execute_reply.started":"2024-11-30T00:08:09.557934Z","shell.execute_reply":"2024-11-30T00:08:21.054416Z"}},"outputs":[{"name":"stdout","text":"DONE\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import cv2\nimport os\nimport random\nfrom tqdm import tqdm\n\n# Define paths for validation folders\nval_violence_path = \"/kaggle/input/rwf2000/RWF-2000/val/Fight\"\nval_nonviolence_path = \"/kaggle/input/rwf2000/RWF-2000/val/NonFight\"\n\n# Output paths for extracted frames\noutput_paths = {\n    \"train_violence\": \"/kaggle/working/extracted_frames/train/violence\",\n    \"train_nonviolence\": \"/kaggle/working/extracted_frames/train/nonviolence\",\n    \"val_violence\": \"/kaggle/working/extracted_frames/val/violence\",\n    \"val_nonviolence\": \"/kaggle/working/extracted_frames/val/nonviolence\"\n}\n\n# Create output directories if they don't exist\nfor path in output_paths.values():\n    os.makedirs(path, exist_ok=True)\n\n# Function to extract frames at a rate of 3 frames per second\ndef extract_frames(video_path, output_folder, target_fps=3):\n    cap = cv2.VideoCapture(video_path)\n    if not cap.isOpened():\n        print(f\"Error opening video file: {video_path}\")\n        return\n    \n    original_fps = cap.get(cv2.CAP_PROP_FPS)\n    frame_interval = int(original_fps / target_fps)\n    frame_count, saved_frame_count = 0, 0\n    \n    while cap.isOpened():\n        ret, frame = cap.read()\n        if not ret:\n            break\n        if frame_count % frame_interval == 0:\n            frame_filename = f\"{os.path.splitext(os.path.basename(video_path))[0]}_frame{saved_frame_count}.jpg\"\n            cv2.imwrite(os.path.join(output_folder, frame_filename), frame)\n            saved_frame_count += 1\n        frame_count += 1\n    cap.release()\n\n# Function to process videos, splitting 10% for validation if needed\ndef process_videos(video_paths, train_output, val_output, split_for_val=False, target_fps=3):\n    for folder in video_paths:\n        video_files = [f for f in os.listdir(folder) if f.lower().endswith(('.mp4', '.avi', '.mov', '.mkv'))]\n        \n        if split_for_val:\n            random.shuffle(video_files)\n            val_count = max(1, int(len(video_files) * 0.2))  # Take 10% for validation\n            val_videos, train_videos = video_files[:val_count], video_files[val_count:]\n        else:\n            val_videos, train_videos = [], video_files  # All to training if no split\n        \n        # Extract frames from validation videos\n        for video_file in tqdm(val_videos, desc=f\"Extracting validation frames from {folder}\"):\n            extract_frames(os.path.join(folder, video_file), val_output, target_fps=target_fps)\n\n        # Extract frames from training videos\n        for video_file in tqdm(train_videos, desc=f\"Extracting training frames from {folder}\"):\n            extract_frames(os.path.join(folder, video_file), train_output, target_fps=target_fps)\n\n# Process violence and nonviolence folders with a 10% split for validation\nprocess_videos(\n    [\"/kaggle/input/real-life-violence-situations-dataset/Real Life Violence Dataset/Violence\"],\n    output_paths[\"train_violence\"],\n    output_paths[\"val_violence\"],\n    split_for_val=True\n)\nprocess_videos(\n    [\"/kaggle/input/real-life-violence-situations-dataset/Real Life Violence Dataset/NonViolence\"],\n    output_paths[\"train_nonviolence\"],\n    output_paths[\"val_nonviolence\"],\n    split_for_val=True\n)\n\n# Process all files from RWF-2000 train folders as training data (no validation split)\nprocess_videos(\n    [\"/kaggle/input/rwf2000/RWF-2000/train/Fight\"],\n    output_paths[\"train_violence\"],\n    output_paths[\"val_violence\"],\n    split_for_val=True\n)\nprocess_videos(\n    [\"/kaggle/input/rwf2000/RWF-2000/train/NonFight\"],\n    output_paths[\"train_nonviolence\"],\n    output_paths[\"val_nonviolence\"],\n    split_for_val=True\n)\n\n\nprint(\"Frame extraction completed.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T00:08:21.057051Z","iopub.execute_input":"2024-11-30T00:08:21.057703Z","iopub.status.idle":"2024-11-30T00:20:52.219744Z","shell.execute_reply.started":"2024-11-30T00:08:21.057660Z","shell.execute_reply":"2024-11-30T00:20:52.218890Z"}},"outputs":[{"name":"stderr","text":"Extracting validation frames from /kaggle/input/real-life-violence-situations-dataset/Real Life Violence Dataset/Violence: 100%|██████████| 200/200 [00:49<00:00,  4.04it/s]\nExtracting training frames from /kaggle/input/real-life-violence-situations-dataset/Real Life Violence Dataset/Violence: 100%|██████████| 800/800 [02:16<00:00,  5.87it/s]\nExtracting validation frames from /kaggle/input/real-life-violence-situations-dataset/Real Life Violence Dataset/NonViolence: 100%|██████████| 200/200 [00:15<00:00, 12.93it/s]\nExtracting training frames from /kaggle/input/real-life-violence-situations-dataset/Real Life Violence Dataset/NonViolence:  82%|████████▏ | 653/800 [01:04<00:11, 12.87it/s][h264 @ 0x56f729ed60c0] mb_type 104 in P slice too large at 98 31\n[h264 @ 0x56f729ed60c0] error while decoding MB 98 31\nExtracting training frames from /kaggle/input/real-life-violence-situations-dataset/Real Life Violence Dataset/NonViolence: 100%|██████████| 800/800 [01:20<00:00,  9.99it/s]\nExtracting validation frames from /kaggle/input/rwf2000/RWF-2000/train/Fight: 100%|██████████| 160/160 [00:53<00:00,  2.99it/s]\nExtracting training frames from /kaggle/input/rwf2000/RWF-2000/train/Fight: 100%|██████████| 640/640 [03:36<00:00,  2.95it/s]\nExtracting validation frames from /kaggle/input/rwf2000/RWF-2000/train/NonFight: 100%|██████████| 160/160 [00:36<00:00,  4.33it/s]\nExtracting training frames from /kaggle/input/rwf2000/RWF-2000/train/NonFight: 100%|██████████| 640/640 [02:41<00:00,  3.95it/s]","output_type":"stream"},{"name":"stdout","text":"Frame extraction completed.\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import os\n\n# Specify the path to the main directory\nmain_dir = '/kaggle/working/extracted_frames'\n\n# Dictionary to store file counts\nfile_counts = {}\n\n# Walk through each subdirectory and count files\nfor root, dirs, files in os.walk(main_dir):\n    # Only count files, skip directories\n    file_counts[root] = len(files)\n\n# Print the file counts in each folder\nfor folder, count in file_counts.items():\n    print(f\"{folder}: {count} files\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T00:20:52.221710Z","iopub.execute_input":"2024-11-30T00:20:52.222125Z","iopub.status.idle":"2024-11-30T00:20:52.276417Z","shell.execute_reply.started":"2024-11-30T00:20:52.222083Z","shell.execute_reply":"2024-11-30T00:20:52.275613Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/extracted_frames: 0 files\n/kaggle/working/extracted_frames/train: 0 files\n/kaggle/working/extracted_frames/train/violence: 22621 files\n/kaggle/working/extracted_frames/train/nonviolence: 23097 files\n/kaggle/working/extracted_frames/val: 0 files\n/kaggle/working/extracted_frames/val/violence: 6706 files\n/kaggle/working/extracted_frames/val/nonviolence: 5689 files\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# Paths to augmented frames for training and validation\ntrain_dir = '/kaggle/working/extracted_frames/train'\nval_dir = '/kaggle/working/extracted_frames/val'\n# Image size and batch size\nIMG_SIZE = (224, 224)\nBATCH_SIZE = 16\n\n\n\"\"\"\n   ,\n    rotation_range=5,              # Small rotation, up to 5 degrees\n    width_shift_range=0.05,        # Small horizontal shift, up to 5% of width\n    height_shift_range=0.05,       # Small vertical shift, up to 5% of height\n    zoom_range=0.1,                # Slight zoom variation, up to 10%\n    fill_mode='nearest' \n    \"\"\"\n# Data augmentation and generators\ntrain_datagen = ImageDataGenerator(\n    rescale=1.0/255,\n    rotation_range=20,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True,\n    brightness_range=[0.8, 1.2],\n    fill_mode='nearest'  \n)\n\n\nval_datagen = ImageDataGenerator(rescale=1.0/255)\n\ntrain_generator = train_datagen.flow_from_directory(\n    train_dir,\n    target_size=IMG_SIZE,\n    batch_size=BATCH_SIZE,\n    class_mode='binary',\n    shuffle= True ,\n)\n\nval_generator = val_datagen.flow_from_directory(\n    val_dir,\n    target_size=IMG_SIZE,\n    batch_size=BATCH_SIZE,\n    class_mode='binary',\n    shuffle=False,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T00:20:52.277719Z","iopub.execute_input":"2024-11-30T00:20:52.278154Z","iopub.status.idle":"2024-11-30T00:20:53.039620Z","shell.execute_reply.started":"2024-11-30T00:20:52.278115Z","shell.execute_reply":"2024-11-30T00:20:53.038803Z"}},"outputs":[{"name":"stdout","text":"Found 45718 images belonging to 2 classes.\nFound 12395 images belonging to 2 classes.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"from tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout, BatchNormalization, Activation\nfrom tensorflow.keras.applications import MobileNetV3Small\nfrom tensorflow.keras import Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.regularizers import l2\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\n\n# Define the MobileNetV3Small model\ninput_tensor = Input(shape=(IMG_SIZE[0], IMG_SIZE[1], 3))\nbase_model = MobileNetV3Small(input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3), include_top=False, weights='imagenet')\n\n\n\nbase_model.trainable = True  # Initially freeze the entire base model\n\"\"\"# Unfreeze the top 10 layers\nfor layer in base_model.layers[-30:]:\n    layer.trainable = True\n    , kernel_regularizer=l2(0.01)\n\"\"\"\n    \n\n# Add custom layers on top of MobileNetV3Small\nx = base_model(input_tensor)\nx = GlobalAveragePooling2D()(x)\n\nx = Dense(1024)(x)\nx = Activation('relu')(x)\nx = Dropout(0.5)(x)  # Add Dropout\n\nx = Dense(512)(x)\nx = Activation('relu')(x)\nx = Dropout(0.5)(x)  # Add Dropout\n\noutput = Dense(1, activation='sigmoid')(x)\n\n# Build the model\nmodel = models.Model(inputs=input_tensor, outputs=output)\n\nmodel.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T03:00:33.703967Z","iopub.execute_input":"2024-11-30T03:00:33.704313Z","iopub.status.idle":"2024-11-30T03:00:34.336006Z","shell.execute_reply.started":"2024-11-30T03:00:33.704284Z","shell.execute_reply":"2024-11-30T03:00:34.334834Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"functional_5\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_5\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ input_layer_4 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ MobilenetV3small (\u001b[38;5;33mFunctional\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m576\u001b[0m)      │       \u001b[38;5;34m939,120\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ global_average_pooling2d_2      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m576\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │       \u001b[38;5;34m590,848\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ activation_58 (\u001b[38;5;33mActivation\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │       \u001b[38;5;34m524,800\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ activation_59 (\u001b[38;5;33mActivation\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_8 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m513\u001b[0m │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ input_layer_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ MobilenetV3small (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">576</span>)      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">939,120</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ global_average_pooling2d_2      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">576</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │       <span style=\"color: #00af00; text-decoration-color: #00af00\">590,848</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ activation_58 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">524,800</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ activation_59 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">513</span> │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,055,281\u001b[0m (7.84 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,055,281</span> (7.84 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,043,169\u001b[0m (7.79 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,043,169</span> (7.79 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m12,112\u001b[0m (47.31 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">12,112</span> (47.31 KB)\n</pre>\n"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"from tensorflow.keras.callbacks import ModelCheckpoint \n\nmodel.compile(optimizer=Adam(learning_rate=1e-4),  # Lower LR for fine-tuning\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\n\n# Callbacks for adaptive learning rate and early stopping\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-5)  \n#early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n        \ncheckpoint = ModelCheckpoint(\n    filepath='best_model.keras',  # Path to save the best model\n    monitor='val_loss',       # Metric to monitor for improvement\n    save_best_only=True,      # Save only if the model improves\n    mode='min',               # Mode: 'min' for loss, 'max' for accuracy\n    verbose=1                 # Print messages when saving\n)\n\nhistory = model.fit(\n    train_generator,\n    validation_data=val_generator,\n    epochs=30,\n    callbacks=[checkpoint,reduce_lr]\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T03:05:09.509814Z","iopub.execute_input":"2024-11-30T03:05:09.510214Z","iopub.status.idle":"2024-11-30T08:16:23.336622Z","shell.execute_reply.started":"2024-11-30T03:05:09.510184Z","shell.execute_reply":"2024-11-30T08:16:23.335916Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/30\n\u001b[1m1948/2858\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m3:18\u001b[0m 218ms/step - accuracy: 0.7333 - loss: 0.5169","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1732936393.125246    8190 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'input_reduce_fusion', 36 bytes spill stores, 36 bytes spill loads\n\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m2858/2858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 214ms/step - accuracy: 0.7547 - loss: 0.4849\nEpoch 1: val_loss improved from inf to 2.30799, saving model to best_model.keras\n\u001b[1m2858/2858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m705s\u001b[0m 226ms/step - accuracy: 0.7547 - loss: 0.4849 - val_accuracy: 0.4590 - val_loss: 2.3080 - learning_rate: 1.0000e-04\nEpoch 2/30\n\u001b[1m2856/2858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 206ms/step - accuracy: 0.8818 - loss: 0.2660\nEpoch 2: val_loss improved from 2.30799 to 1.59476, saving model to best_model.keras\n\u001b[1m2858/2858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m621s\u001b[0m 217ms/step - accuracy: 0.8818 - loss: 0.2659 - val_accuracy: 0.5803 - val_loss: 1.5948 - learning_rate: 1.0000e-04\nEpoch 3/30\n\u001b[1m2856/2858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 206ms/step - accuracy: 0.9162 - loss: 0.1971\nEpoch 3: val_loss improved from 1.59476 to 0.87873, saving model to best_model.keras\n\u001b[1m2858/2858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m620s\u001b[0m 216ms/step - accuracy: 0.9162 - loss: 0.1971 - val_accuracy: 0.6623 - val_loss: 0.8787 - learning_rate: 1.0000e-04\nEpoch 4/30\n\u001b[1m2858/2858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 205ms/step - accuracy: 0.9356 - loss: 0.1548\nEpoch 4: val_loss did not improve from 0.87873\n\u001b[1m2858/2858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m617s\u001b[0m 215ms/step - accuracy: 0.9356 - loss: 0.1548 - val_accuracy: 0.6738 - val_loss: 0.9774 - learning_rate: 1.0000e-04\nEpoch 5/30\n\u001b[1m2858/2858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 206ms/step - accuracy: 0.9469 - loss: 0.1264\nEpoch 5: val_loss did not improve from 0.87873\n\u001b[1m2858/2858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m622s\u001b[0m 217ms/step - accuracy: 0.9469 - loss: 0.1264 - val_accuracy: 0.4657 - val_loss: 4.1677 - learning_rate: 1.0000e-04\nEpoch 6/30\n\u001b[1m2857/2858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 207ms/step - accuracy: 0.9549 - loss: 0.1093\nEpoch 6: val_loss did not improve from 0.87873\n\u001b[1m2858/2858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m624s\u001b[0m 218ms/step - accuracy: 0.9549 - loss: 0.1093 - val_accuracy: 0.4751 - val_loss: 3.2286 - learning_rate: 1.0000e-04\nEpoch 7/30\n\u001b[1m2858/2858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 207ms/step - accuracy: 0.9632 - loss: 0.0944\nEpoch 7: val_loss did not improve from 0.87873\n\u001b[1m2858/2858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m624s\u001b[0m 218ms/step - accuracy: 0.9632 - loss: 0.0944 - val_accuracy: 0.7651 - val_loss: 0.8843 - learning_rate: 1.0000e-04\nEpoch 8/30\n\u001b[1m2857/2858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 207ms/step - accuracy: 0.9686 - loss: 0.0844\nEpoch 8: val_loss did not improve from 0.87873\n\u001b[1m2858/2858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m623s\u001b[0m 217ms/step - accuracy: 0.9686 - loss: 0.0844 - val_accuracy: 0.7718 - val_loss: 1.1968 - learning_rate: 1.0000e-04\nEpoch 9/30\n\u001b[1m2856/2858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 207ms/step - accuracy: 0.9732 - loss: 0.0665\nEpoch 9: val_loss improved from 0.87873 to 0.67230, saving model to best_model.keras\n\u001b[1m2858/2858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m623s\u001b[0m 218ms/step - accuracy: 0.9732 - loss: 0.0665 - val_accuracy: 0.8723 - val_loss: 0.6723 - learning_rate: 5.0000e-05\nEpoch 10/30\n\u001b[1m2858/2858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 206ms/step - accuracy: 0.9774 - loss: 0.0561\nEpoch 10: val_loss did not improve from 0.67230\n\u001b[1m2858/2858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m622s\u001b[0m 217ms/step - accuracy: 0.9774 - loss: 0.0561 - val_accuracy: 0.6515 - val_loss: 1.5434 - learning_rate: 5.0000e-05\nEpoch 11/30\n\u001b[1m2857/2858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 206ms/step - accuracy: 0.9783 - loss: 0.0560\nEpoch 11: val_loss did not improve from 0.67230\n\u001b[1m2858/2858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m620s\u001b[0m 217ms/step - accuracy: 0.9783 - loss: 0.0560 - val_accuracy: 0.5964 - val_loss: 2.9611 - learning_rate: 5.0000e-05\nEpoch 12/30\n\u001b[1m2856/2858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 206ms/step - accuracy: 0.9820 - loss: 0.0499\nEpoch 12: val_loss did not improve from 0.67230\n\u001b[1m2858/2858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m620s\u001b[0m 217ms/step - accuracy: 0.9820 - loss: 0.0499 - val_accuracy: 0.8444 - val_loss: 0.9076 - learning_rate: 5.0000e-05\nEpoch 13/30\n\u001b[1m2858/2858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 208ms/step - accuracy: 0.9837 - loss: 0.0431\nEpoch 13: val_loss did not improve from 0.67230\n\u001b[1m2858/2858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m626s\u001b[0m 219ms/step - accuracy: 0.9837 - loss: 0.0431 - val_accuracy: 0.8070 - val_loss: 0.9909 - learning_rate: 5.0000e-05\nEpoch 14/30\n\u001b[1m2856/2858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 207ms/step - accuracy: 0.9816 - loss: 0.0490\nEpoch 14: val_loss did not improve from 0.67230\n\u001b[1m2858/2858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m624s\u001b[0m 218ms/step - accuracy: 0.9816 - loss: 0.0490 - val_accuracy: 0.8348 - val_loss: 1.0098 - learning_rate: 5.0000e-05\nEpoch 15/30\n\u001b[1m2858/2858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 208ms/step - accuracy: 0.9852 - loss: 0.0387\nEpoch 15: val_loss did not improve from 0.67230\n\u001b[1m2858/2858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m626s\u001b[0m 218ms/step - accuracy: 0.9852 - loss: 0.0387 - val_accuracy: 0.8866 - val_loss: 0.7115 - learning_rate: 2.5000e-05\nEpoch 16/30\n\u001b[1m2858/2858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 207ms/step - accuracy: 0.9870 - loss: 0.0345\nEpoch 16: val_loss did not improve from 0.67230\n\u001b[1m2858/2858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m625s\u001b[0m 218ms/step - accuracy: 0.9870 - loss: 0.0345 - val_accuracy: 0.8896 - val_loss: 0.7991 - learning_rate: 2.5000e-05\nEpoch 17/30\n\u001b[1m2856/2858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 206ms/step - accuracy: 0.9877 - loss: 0.0316\nEpoch 17: val_loss did not improve from 0.67230\n\u001b[1m2858/2858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m620s\u001b[0m 216ms/step - accuracy: 0.9877 - loss: 0.0316 - val_accuracy: 0.8813 - val_loss: 0.7821 - learning_rate: 2.5000e-05\nEpoch 18/30\n\u001b[1m2856/2858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 205ms/step - accuracy: 0.9884 - loss: 0.0304\nEpoch 18: val_loss did not improve from 0.67230\n\u001b[1m2858/2858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m618s\u001b[0m 216ms/step - accuracy: 0.9884 - loss: 0.0304 - val_accuracy: 0.8805 - val_loss: 0.7737 - learning_rate: 2.5000e-05\nEpoch 19/30\n\u001b[1m2858/2858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 205ms/step - accuracy: 0.9890 - loss: 0.0302\nEpoch 19: val_loss did not improve from 0.67230\n\u001b[1m2858/2858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m617s\u001b[0m 215ms/step - accuracy: 0.9890 - loss: 0.0302 - val_accuracy: 0.8926 - val_loss: 0.7089 - learning_rate: 2.5000e-05\nEpoch 20/30\n\u001b[1m2857/2858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 205ms/step - accuracy: 0.9896 - loss: 0.0270\nEpoch 20: val_loss did not improve from 0.67230\n\u001b[1m2858/2858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m618s\u001b[0m 216ms/step - accuracy: 0.9896 - loss: 0.0270 - val_accuracy: 0.8294 - val_loss: 1.0140 - learning_rate: 1.2500e-05\nEpoch 21/30\n\u001b[1m2856/2858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 206ms/step - accuracy: 0.9901 - loss: 0.0258\nEpoch 21: val_loss did not improve from 0.67230\n\u001b[1m2858/2858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m618s\u001b[0m 216ms/step - accuracy: 0.9901 - loss: 0.0258 - val_accuracy: 0.8903 - val_loss: 0.8936 - learning_rate: 1.2500e-05\nEpoch 22/30\n\u001b[1m2856/2858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 207ms/step - accuracy: 0.9897 - loss: 0.0265\nEpoch 22: val_loss did not improve from 0.67230\n\u001b[1m2858/2858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m623s\u001b[0m 217ms/step - accuracy: 0.9897 - loss: 0.0265 - val_accuracy: 0.8976 - val_loss: 0.7970 - learning_rate: 1.2500e-05\nEpoch 23/30\n\u001b[1m2856/2858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 206ms/step - accuracy: 0.9905 - loss: 0.0238\nEpoch 23: val_loss did not improve from 0.67230\n\u001b[1m2858/2858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m621s\u001b[0m 217ms/step - accuracy: 0.9905 - loss: 0.0238 - val_accuracy: 0.8503 - val_loss: 0.9225 - learning_rate: 1.2500e-05\nEpoch 24/30\n\u001b[1m2856/2858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 204ms/step - accuracy: 0.9913 - loss: 0.0233\nEpoch 24: val_loss did not improve from 0.67230\n\u001b[1m2858/2858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m613s\u001b[0m 214ms/step - accuracy: 0.9913 - loss: 0.0234 - val_accuracy: 0.8994 - val_loss: 0.7564 - learning_rate: 1.2500e-05\nEpoch 25/30\n\u001b[1m2857/2858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 203ms/step - accuracy: 0.9910 - loss: 0.0239\nEpoch 25: val_loss did not improve from 0.67230\n\u001b[1m2858/2858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m610s\u001b[0m 213ms/step - accuracy: 0.9910 - loss: 0.0239 - val_accuracy: 0.8871 - val_loss: 0.8619 - learning_rate: 1.0000e-05\nEpoch 26/30\n\u001b[1m2856/2858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 203ms/step - accuracy: 0.9912 - loss: 0.0228\nEpoch 26: val_loss did not improve from 0.67230\n\u001b[1m2858/2858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m612s\u001b[0m 214ms/step - accuracy: 0.9912 - loss: 0.0228 - val_accuracy: 0.8937 - val_loss: 0.8397 - learning_rate: 1.0000e-05\nEpoch 27/30\n\u001b[1m2856/2858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 204ms/step - accuracy: 0.9924 - loss: 0.0198\nEpoch 27: val_loss did not improve from 0.67230\n\u001b[1m2858/2858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m613s\u001b[0m 214ms/step - accuracy: 0.9924 - loss: 0.0198 - val_accuracy: 0.8967 - val_loss: 0.8084 - learning_rate: 1.0000e-05\nEpoch 28/30\n\u001b[1m2858/2858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 204ms/step - accuracy: 0.9928 - loss: 0.0194\nEpoch 28: val_loss did not improve from 0.67230\n\u001b[1m2858/2858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m614s\u001b[0m 214ms/step - accuracy: 0.9928 - loss: 0.0194 - val_accuracy: 0.9013 - val_loss: 0.7739 - learning_rate: 1.0000e-05\nEpoch 29/30\n\u001b[1m2858/2858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 206ms/step - accuracy: 0.9912 - loss: 0.0220\nEpoch 29: val_loss did not improve from 0.67230\n\u001b[1m2858/2858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m620s\u001b[0m 216ms/step - accuracy: 0.9912 - loss: 0.0220 - val_accuracy: 0.8993 - val_loss: 0.8485 - learning_rate: 1.0000e-05\nEpoch 30/30\n\u001b[1m2858/2858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 205ms/step - accuracy: 0.9923 - loss: 0.0195\nEpoch 30: val_loss did not improve from 0.67230\n\u001b[1m2858/2858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m615s\u001b[0m 215ms/step - accuracy: 0.9923 - loss: 0.0195 - val_accuracy: 0.8944 - val_loss: 0.9060 - learning_rate: 1.0000e-05\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"from sklearn.metrics import classification_report\nimport numpy as np\n\n# Step 1: Evaluate the model using the validation generator\nloss, accuracy = model.evaluate(val_generator, verbose=1)\nprint(f\"Validation Loss: {loss}\")\nprint(f\"Validation Accuracy: {accuracy}\")\n\n# Step 2: Generate predictions\npredictions = model.predict(val_generator, verbose=1)          \npredicted_classes = (predictions > 0.5).astype(int).reshape(-1)  # Binary classification: Threshold at 0.5\n\n# Step 3: Get true labels from the generator\ntrue_classes = val_generator.classes  # Assuming this provides the true labels for each sample\nclass_labels = ['Class 0', 'Class 1']  # Replace with actual class names if available\n\n# Step 4: Generate classification report\nprint(\"\\nClassification Report:\\n\")\nprint(classification_report(true_classes, predicted_classes, target_names=class_labels))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T08:16:23.338915Z","iopub.execute_input":"2024-11-30T08:16:23.339298Z","iopub.status.idle":"2024-11-30T08:17:29.502014Z","shell.execute_reply.started":"2024-11-30T08:16:23.339249Z","shell.execute_reply":"2024-11-30T08:17:29.501158Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m775/775\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 39ms/step - accuracy: 0.8730 - loss: 1.0437\nValidation Loss: 0.9060072302818298\nValidation Accuracy: 0.8943929076194763\n\u001b[1m775/775\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 41ms/step\n\nClassification Report:\n\n              precision    recall  f1-score   support\n\n     Class 0       0.90      0.86      0.88      5689\n     Class 1       0.89      0.92      0.90      6706\n\n    accuracy                           0.89     12395\n   macro avg       0.90      0.89      0.89     12395\nweighted avg       0.89      0.89      0.89     12395\n\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# Save the model in .keras format\nmodel.save(\"movileV3_89.keras\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T08:20:12.422014Z","iopub.execute_input":"2024-11-30T08:20:12.423567Z","iopub.status.idle":"2024-11-30T08:20:12.892257Z","shell.execute_reply.started":"2024-11-30T08:20:12.423511Z","shell.execute_reply":"2024-11-30T08:20:12.891274Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"pip install tf2onnx\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T08:21:15.960079Z","iopub.execute_input":"2024-11-30T08:21:15.960906Z","iopub.status.idle":"2024-11-30T08:21:26.023446Z","shell.execute_reply.started":"2024-11-30T08:21:15.960844Z","shell.execute_reply":"2024-11-30T08:21:26.022279Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"Collecting tf2onnx\n  Downloading tf2onnx-1.16.1-py3-none-any.whl.metadata (1.3 kB)\nRequirement already satisfied: numpy>=1.14.1 in /opt/conda/lib/python3.10/site-packages (from tf2onnx) (1.26.4)\nRequirement already satisfied: onnx>=1.4.1 in /opt/conda/lib/python3.10/site-packages (from tf2onnx) (1.17.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from tf2onnx) (2.32.3)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from tf2onnx) (1.16.0)\nRequirement already satisfied: flatbuffers>=1.12 in /opt/conda/lib/python3.10/site-packages (from tf2onnx) (24.3.25)\nRequirement already satisfied: protobuf~=3.20 in /opt/conda/lib/python3.10/site-packages (from tf2onnx) (3.20.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->tf2onnx) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->tf2onnx) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->tf2onnx) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->tf2onnx) (2024.8.30)\nDownloading tf2onnx-1.16.1-py3-none-any.whl (455 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m455.8/455.8 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: tf2onnx\nSuccessfully installed tf2onnx-1.16.1\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"import tf2onnx\nimport tensorflow as tf\n\n# Assuming 'model' is your trained Keras or TensorFlow model\n\n# Define the path to save the ONNX model\nonnx_model_path = \"movileV3_89.onnx\"\n\n# Convert the model to ONNX format\nspec = (tf.TensorSpec(model.inputs[0].shape, tf.float32, name=\"input\"),)\nonnx_model, _ = tf2onnx.convert.from_keras(model, input_signature=spec)\n\n# Save the ONNX model to the specified path\nwith open(onnx_model_path, \"wb\") as f:\n    f.write(onnx_model.SerializeToString())\n\nprint(f\"Model saved in ONNX format at {onnx_model_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T08:21:26.025353Z","iopub.execute_input":"2024-11-30T08:21:26.025662Z","iopub.status.idle":"2024-11-30T08:21:31.115692Z","shell.execute_reply.started":"2024-11-30T08:21:26.025634Z","shell.execute_reply":"2024-11-30T08:21:31.114762Z"}},"outputs":[{"name":"stdout","text":"Model saved in ONNX format at movileV3_89.onnx\n","output_type":"stream"}],"execution_count":18}]}